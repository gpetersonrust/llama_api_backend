<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>semantic_search.ipynb – Notebook Outline</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #f8f9fa;
      color: #212529;
      line-height: 1.6;
      margin: 40px;
    }

    hr{
      border: 0;
      height: 1px;
      background-color: #dee2e6;
      margin: 40px 0;
    }

    h1, h2 {
      color: #343a40;
    }

    h1 {
      font-size: 2rem;
      margin-bottom: 1rem;
      border-bottom: 2px solid #dee2e6;
      padding-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.5rem;
      margin-top: 2rem;
      margin-bottom: 0.5rem;
    }

    p, li {
      font-size: 1rem;
    }

    pre {
      background-color: #e9ecef;
      padding: 10px;
      border-left: 4px solid #adb5bd;
      overflow-x: auto;
    }

    a {
      color: #007bff;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    ul {
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    code {
      background: #f1f3f5;
      padding: 2px 4px;
      border-radius: 4px;
      font-family: monospace;
    }
  </style>
</head>
<body>

  <h1>📘 semantic_search.ipynb – Notebook Outline</h1>

  <h2>🧠 Project Context</h2>
  <p>
    You’re building a retrieval-augmented chatbot using FastAPI, Ollama, FAISS, and LangChain.
    The system is designed to return pre-approved answers using semantic search (FAISS),
    then rerank the top results using a lightweight local BM25 scorer for higher precision.
    If no relevant answer is found, it silently falls back to generating a new response using
    Ollama’s local LLaMA3 (8B) model. This notebook walks through the full RAG pipeline from
    keyword extraction to reranking and fallback generation, before it’s integrated into
    <code>llama_api_backend-main.zip</code>.
  </p>

  <h2>📑 Table of Contents (with Docs + Function Suggestions)</h2>

  <h3>1. Setup & Environment</h3>
  <ul>
    <li>Install packages, import modules, create <code>Stores/</code></li>
    <li><a href="https://python.langchain.com/docs/get_started/installation" target="_blank">LangChain Installation</a></li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" target="_blank">FAISS Setup</a></li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/chroma" target="_blank">Chroma Setup</a></li>
    <pre><code>def setup_environment(): ...</code></pre>
  </ul>

  <h3>2. Keyword Extraction</h3>
  <ul>
    <li>Use Ollama 2B model to extract keywords</li>
    <li><a href="https://ollama.com/library" target="_blank">Ollama Library</a></li>
    <li><a href="https://python.langchain.com/docs/modules/model_io/llms/" target="_blank">LangChain LLMs</a></li>
    <pre><code>def extract_keywords_from_prompt(prompt: str) -> list: ...</code></pre>
  </ul>

  <h3>3. Load, Chunk & Index Q&A Data (FAISS)</h3>
  <ul>
    <li>Load manually tagged Q&A JSON files</li>
    <li>Chunk long answers using token-aware splitting</li>
    <li>Embed chunks using <code>nomic-embed-text</code> via Ollama</li>
    <li>Save to FAISS vector store (semantic search)</li>
    <li>If index already exists, load instead of rebuilding</li>
    <pre><code>class QADocumentIndexer:
      def load_or_create_faiss() -> FAISS: ...
  </code></pre>
  </ul>
  
  <h3>4. Query FAISS</h3>
  <ul>
    <li>Use FAISS to retrieve top-k semantically similar documents</li>
    <li>Apply similarity threshold filter</li>
    <li>Return scored candidates for reranking</li>
    <pre><code>def query_faiss(query: str, k: int = 5): ...</code></pre>
  </ul>
  
  <h3>5. BM25-Based Reranking</h3>
  <ul>
    <li>Apply local BM25 reranker using <code>rank_bm25</code> or similar</li>
    <li>Score top FAISS chunks based on keyword overlap</li>
    <li>Sort and return top result(s)</li>
    <pre><code>def rerank_with_bm25(query: str, faiss_docs: list) -> list: ...</code></pre>
  </ul>
  
  <h3>6. Final Answer Selection</h3>
  <ul>
    <li>Choose top reranked answer</li>
    <li>If none exceed threshold, fall back to LLaMA3</li>
    <li>Return result + metadata (source: knowledge_base | generated)</li>
  </ul>
  
  <h3>7. Fallback to LLaMA3</h3>
  <ul>
    <li>Send prompt + history to <code>ollama.chat()</code></li>
    <li>Use default system prompt + chat threading</li>
    <li>Return assistant response</li>
  </ul>
  
  <h3>8. Save Metadata</h3>
  <ul>
    <li>Log extracted keywords and unanswered prompts</li>
    <li>Append to <code>extracted_keywords.json</code> and <code>need_answers.json</code></li>
  </ul>
  
  <h3>9. Route Final Answer</h3>
  <ul>
    <li>Return response to frontend with source tag</li>
    <li>Include extracted keywords for UI enhancement</li>
  </ul>
  
  <h3>10. Evaluate & Expand Q&A JSON</h3>
  <ul>
    <li>Review <code>need_answers.json</code></li>
    <li>Manually expand Q&A dataset</li>
    <li>Mark approved entries and re-index</li>
  </ul>

  <hr />

  <h2>🧱 1. Setup & Environment</h2>

<p>
  Before anything else, we need to prepare our environment to support the full RAG pipeline. This includes installing core dependencies, importing required libraries, and preparing the <code>Stores/</code> directory to hold our persistent vector databases (FAISS and Chroma).
</p>

<h3>📦 Required Dependencies</h3>

<ul>
  <li><code>langchain==0.1.13</code> – Framework for chaining LLMs, vector search, and document management</li>
  <li><code>faiss-cpu==1.7.4</code> – Vector similarity search engine used for dense semantic retrieval</li>
  <li><code>chromadb==0.4.22</code> – Lightweight keyword/BM25-style vector database for sparse retrieval</li>
  <li><code>ollama==0.1.8</code> – Interface for local LLaMA3-based models (inference and embedding)</li>
  <li><code>tiktoken==0.5.1</code> – Tokenization utility for model-compatible chunking</li>
  <li><code>openai==1.12.0</code> – Required internally by LangChain for embedding API compatibility</li>
  <li><code>python-dotenv==1.0.1</code> – Load secrets and model names from environment variables</li>
  <li><code>nltk>=3.8.1</code> – (Optional) for text preprocessing and keyword utility</li>
  <li><code>scikit-learn</code> – Used internally for similarity math (e.g., cosine scores)</li>
  <li><code>transformers</code> – (Optional) for reranking or small-model processing if needed</li>
</ul>

<h3>🔧 Installation</h3>

<pre><code>pip install langchain==0.1.13 faiss-cpu==1.7.4 chromadb==0.4.22 \ 
    tiktoken==0.5.1 openai==1.12.0 python-dotenv==1.0.1 \
    nltk scikit-learn transformers ollama==0.1.8
</code></pre>

<h3>🗂️ Directory Structure</h3>

<p>
  Ensure your project includes a <code>Stores/</code> folder where FAISS and Chroma index files will be persisted between runs.
</p>

<pre><code>llama_api_backend/
├── Stores/
│   ├── faiss_index/
│   └── chroma_index/
├── semantic_search.ipynb
├── app/
│   └── ...
</code></pre>

<h3>🔁 Function Template</h3>

<pre><code>def setup_environment():
    import os

    # Create necessary directories for vector stores
    os.makedirs("Stores/faiss_index", exist_ok=True)
    os.makedirs("Stores/chroma_index", exist_ok=True)

    print("✅ Environment ready – vector store folders created.")
</code></pre>

<h3>🔗 Helpful Links</h3>
<ul>
  <li><a href="https://python.langchain.com/docs/get_started/installation" target="_blank">LangChain Installation Guide</a></li>
  <li><a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" target="_blank">FAISS Vector Store Docs</a></li>
  <li><a href="https://python.langchain.com/docs/integrations/vectorstores/chroma" target="_blank">ChromaDB Integration Guide</a></li>
</ul>


<hr />

<h2>🔍 2. Keyword Extraction</h2>

<p>
  In this step, we extract <strong>core keywords or named entities</strong> from the user’s query using a lightweight Ollama model (e.g., 2B). This helps drive downstream processes like:
</p>

<ul>
  <li>Optional BM25-style keyword retrieval (via ChromaDB)</li>
  <li>UI suggestions for pre-populated follow-up questions in WordPress</li>
  <li>Future analytics or relevance scoring</li>
</ul>

<h3>🤖 Why Use a Model for Keywords?</h3>

<p>
  Instead of relying on brittle rule-based extractors, we use a **small LLaMA model via Ollama** to semantically extract high-value tokens, even across diverse domains. This gives us:
</p>

<ul>
  <li>More intelligent phrasing (e.g., detecting “auto insurance” vs “auto” + “insurance”)</li>
  <li>Fewer false positives than pure TF-IDF or regex</li>
  <li>A consistent JSON structure for downstream tools</li>
</ul>

<h3>🧠 System Prompt</h3>

<p>
  The following system prompt guides the model to extract 2–5 of the most important keywords or named entities:
</p>

<pre><code>{
  "role": "system",
  "content": "You are a helpful assistant that extracts 2 to 5 meaningful keywords or named entities from a user prompt. Return them as a Python list of lowercase strings. Avoid stopwords, articles, and general terms."
}
</code></pre>

<h3>🔁 Function Template</h3>

<pre><code>from ollama import chat

def extract_keywords_from_prompt(prompt: str) -> list[str]:
    system_prompt = {
        "role": "system",
        "content": (
            "You are a helpful assistant that extracts 2 to 5 meaningful keywords or named entities "
            "from a user prompt. Return them as a Python list of lowercase strings. "
            "Avoid stopwords, articles, and general terms."
        )
    }

    user_prompt = {"role": "user", "content": prompt}

    try:
        response = chat(
            model="llama2:2b",  # or any small/fast Ollama model
            messages=[system_prompt, user_prompt]
        )

        raw_output = response.get("message", {}).get("content", "[]")
        keywords = eval(raw_output.strip()) if isinstance(raw_output, str) else []

        # Optional: basic sanitization
        return [kw.lower().strip() for kw in keywords if isinstance(kw, str)]

    except Exception as e:
        print(f"❌ Keyword extraction failed: {e}")
        return []
</code></pre>

<h3>💾 Optional Logging</h3>

<p>
  Store the extracted keywords in a local file (e.g., <code>extracted_keywords.json</code>) for:
</p>

<ul>
  <li>Debugging</li>
  <li>Training future ranking models</li>
  <li>WordPress frontend suggestions</li>
</ul>

<pre><code>import json

def save_keywords_to_json(prompt: str, keywords: list[str]):
    log_path = "Stores/extracted_keywords.json"

    try:
        existing = []
        if os.path.exists(log_path):
            with open(log_path, "r") as f:
                existing = json.load(f)

        existing.append({"prompt": prompt, "keywords": keywords})

        with open(log_path, "w") as f:
            json.dump(existing, f, indent=2)

    except Exception as e:
        print(f"⚠️ Could not save keywords: {e}")
</code></pre>

<h3>🔗 References</h3>
<ul>
  <li><a href="https://ollama.com/library" target="_blank">Ollama Model Library</a></li>
  <li><a href="https://python.langchain.com/docs/modules/model_io/llms/" target="_blank">LangChain LLMs (Optional)</a></li>
</ul>

<hr />

<h2>📥 3. Load & Index Q&A Data (FAISS + Chroma)</h2>

<p>
  In this step, we’ll use a custom class to handle loading, chunking, embedding, and indexing our Q&A JSON data into two vector databases:
</p>

<ul>
  <li><strong>FAISS</strong> – for dense semantic search using vector embeddings</li>
  <li><strong>ChromaDB</strong> – for keyword/BM25-based sparse retrieval</li>
</ul>

<h3>🧠 Class-Based Design</h3>

<p>
  We’ll use a Python class called <code>QADocumentIndexer</code> to encapsulate all steps:
</p>

<ul>
  <li>Load and parse Q&A JSON</li>
  <li>Optionally chunk text (for FAISS)</li>
  <li>Embed and store in FAISS</li>
  <li>Store raw documents in Chroma for BM25</li>
</ul>

<hr />

<h3>📦 QADocumentIndexer (Full Code)</h3>

<pre><code>import os
import json
from typing import List
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS, Chroma
from langchain.embeddings import OllamaEmbeddings

class QADocumentIndexer:
    def __init__(self, json_path: str,
                 faiss_dir: str = "Stores/faiss_index",
                 chroma_dir: str = "Stores/chroma_index",
                 embedding_model: str = "nomic-embed-text"):
        self.json_path = json_path
        self.faiss_dir = faiss_dir
        self.chroma_dir = chroma_dir
        self.embedding = OllamaEmbeddings(model=embedding_model)
        self.raw_docs: List[Document] = []
        self.chunked_docs: List[Document] = []
        os.makedirs(faiss_dir, exist_ok=True)
        os.makedirs(chroma_dir, exist_ok=True)

    def load_qa_json(self):
        with open(self.json_path, "r", encoding="utf-8") as f:
            qa_data = json.load(f)
        self.raw_docs = [
            Document(
                page_content=entry["answer"],
                metadata={
                    "questions": entry.get("questions", []),
                    "tags": entry.get("tags", []),
                    "confidence": entry.get("confidence", 0.0),
                    "source": entry.get("source", "unknown"),
                    "approved": entry.get("approved", False)
                }
            ) for entry in qa_data
        ]
        print(f"✅ Loaded {len(self.raw_docs)} raw documents.")

    def chunk_documents(self, chunk_size=512, chunk_overlap=50):
        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        self.chunked_docs = splitter.split_documents(self.raw_docs)
        print(f"✂️ Chunked into {len(self.chunked_docs)} documents for FAISS.")

    def index_to_faiss(self):
        if not self.chunked_docs:
            raise ValueError("No chunks. Run chunk_documents() first.")
        faiss_store = FAISS.from_documents(self.chunked_docs, self.embedding)
        faiss_store.save_local(self.faiss_dir)
        print(f"✅ FAISS index saved to: {self.faiss_dir}")

    def index_to_chroma(self):
        if not self.raw_docs:
            raise ValueError("No raw docs. Run load_qa_json() first.")
        chroma_store = Chroma.from_documents(
            documents=self.raw_docs,
            embedding=None,  # No need for embeddings in BM25
            persist_directory=self.chroma_dir,
            collection_name="qa_bm25"
        )
        chroma_store.persist()
        print(f"✅ ChromaBM25 index saved to: {self.chroma_dir}")

    def run_all(self):
        self.load_qa_json()
        self.chunk_documents()
        self.index_to_faiss()
        self.index_to_chroma()
        print("🚀 Full Q&A indexing completed.")
</code></pre>

<h3>🧪 Usage Example</h3>

<pre><code># One-liner
indexer = QADocumentIndexer("data/sample_qa_pairs.json")
indexer.run_all()

# Or step-by-step
indexer = QADocumentIndexer("data/sample_qa_pairs.json")
indexer.load_qa_json()
indexer.chunk_documents()
indexer.index_to_faiss()
indexer.index_to_chroma()
</code></pre>

<h3>📁 Result</h3>
<p>This class will create the following persistent folders:</p>

<ul>
  <li><code>Stores/faiss_index/</code> → used by FAISS retriever</li>
  <li><code>Stores/chroma_index/</code> → used by Chroma (BM25 keyword search)</li>
</ul>

<h3>🔗 Next Step</h3>
<p>Move on to <strong>Chapter 4</strong> to set up <code>EnsembleRetriever</code> and perform real hybrid searches.</p>


<hr />

<h2>🛑 Project Break Checkpoint</h2>

<h3>✅ Where You Stopped</h3>
<ul>
  <li>Built and indexed semantic Q&A data using <strong>FAISS</strong></li>
  <li>Extracted keywords from user prompts using <strong>Ollama</strong></li>
  <li>Rebuilt <strong>BM25Retriever</strong> from chunked FAISS docs (in-memory)</li>
  <li>Tested how FAISS and BM25 independently score results</li>
  <li>Confirmed that a <strong>custom reranker</strong> can merge both scores using weighted logic</li>
</ul>

<h3>🧠 Key Concepts</h3>
<ul>
  <li><strong>FAISS:</strong> Uses full prompt for dense semantic retrieval</li>
  <li><strong>BM25:</strong> Uses extracted keywords for sparse token overlap</li>
  <li><strong>Reranker:</strong> Custom function to boost docs found in both or weight relevance</li>
</ul>

<h3>🛠️ What You’ll Build Next (Tomorrow)</h3>
<p>You’ll implement a hybrid semantic + keyword search pipeline inside a single function:</p>

<pre><code>def semantic_search_for_previous_answers(prompt: str) -> dict:
    ...
</code></pre>

<p>This function will:</p>
<ol>
  <li>Extract keywords from the prompt (Ollama)</li>
  <li>Run FAISS search on the full prompt</li>
  <li>Run BM25 search on the keywords</li>
  <li>Rerank the results based on a weighted score</li>
  <li>Return the top-scoring answer if confident</li>
  <li>Otherwise, fall back to LLaMA3 via <code>ollama.chat()</code></li>
</ol>

<p>Ready to go live once you return ✨</p>



</body>
</html>