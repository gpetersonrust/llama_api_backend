<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>semantic_search.ipynb â€“ Notebook Outline</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #f8f9fa;
      color: #212529;
      line-height: 1.6;
      margin: 40px;
    }

    hr{
      border: 0;
      height: 1px;
      background-color: #dee2e6;
      margin: 40px 0;
    }

    h1, h2 {
      color: #343a40;
    }

    h1 {
      font-size: 2rem;
      margin-bottom: 1rem;
      border-bottom: 2px solid #dee2e6;
      padding-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.5rem;
      margin-top: 2rem;
      margin-bottom: 0.5rem;
    }

    p, li {
      font-size: 1rem;
    }

    pre {
      background-color: #e9ecef;
      padding: 10px;
      border-left: 4px solid #adb5bd;
      overflow-x: auto;
    }

    a {
      color: #007bff;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    ul {
      margin-top: 0.5rem;
      margin-bottom: 1rem;
    }

    code {
      background: #f1f3f5;
      padding: 2px 4px;
      border-radius: 4px;
      font-family: monospace;
    }
  </style>
</head>
<body>

  <h1>ğŸ“˜ semantic_search.ipynb â€“ Notebook Outline</h1>

  <h2>ğŸ§  Project Context</h2>
  <p>
    Youâ€™re building a retrieval-augmented chatbot using FastAPI, Ollama, FAISS, ChromaDB, and LangChain.
    The goal is to return pre-approved answers whenever possible and only generate new responses using LLaMA3 (8B) if no relevant answer is found.
    This notebook walks you through the full RAG pipeline before itâ€™s added to <code>llama_api_backend-main.zip</code>.
  </p>

  <h2>ğŸ“‘ Table of Contents (with Docs + Function Suggestions)</h2>

  <h3>1. Setup & Environment</h3>
  <ul>
    <li>Install packages, import modules, create <code>Stores/</code></li>
    <li><a href="https://python.langchain.com/docs/get_started/installation" target="_blank">LangChain Installation</a></li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" target="_blank">FAISS Setup</a></li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/chroma" target="_blank">Chroma Setup</a></li>
    <pre><code>def setup_environment(): ...</code></pre>
  </ul>

  <h3>2. Keyword Extraction</h3>
  <ul>
    <li>Use Ollama 2B model to extract keywords</li>
    <li><a href="https://ollama.com/library" target="_blank">Ollama Library</a></li>
    <li><a href="https://python.langchain.com/docs/modules/model_io/llms/" target="_blank">LangChain LLMs</a></li>
    <pre><code>def extract_keywords_from_prompt(prompt: str) -> list: ...</code></pre>
  </ul>

  <h3>3. Load & Inspect Q&A Data</h3>
  <ul>
    <li>Load questions, answers, tags, etc.</li>
    <li><a href="https://python.langchain.com/docs/modules/data_connection/document_loaders/json" target="_blank">LangChain JSON Loader</a></li>
    <pre><code>def load_qa_json(path: str) -> list[dict]: ...</code></pre>
  </ul>

  <h3>4. Token-Aware Text Splitting</h3>
  <ul>
    <li>Use <code>RecursiveCharacterTextSplitter.from_tiktoken_encoder()</code></li>
    <li><a href="https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter" target="_blank">Text Splitter Docs</a></li>
    <pre><code>def chunk_documents(docs: list[dict]) -> list[Document]: ...</code></pre>
  </ul>

  <h3>5. Embedding with nomic-embed-text</h3>
  <ul>
    <li>Embed documents using <code>ollama.Embedding(model="nomic-embed-text")</code></li>
    <li><a href="https://python.langchain.com/docs/guides/embeddings" target="_blank">LangChain Embedding Guide</a></li>
    <pre><code>def embed_documents(docs: list[Document]) -> list: ...</code></pre>
  </ul>

  <h3>6. Vector Store Setup</h3>
  <ul>
    <li>FAISS for semantic â†’ Chroma for keyword/BM25</li>
    <li><a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" target="_blank">FAISS</a></li>
    <li><a href="https://docs.trychroma.com/usage-guide" target="_blank">Chroma Usage Guide</a></li>
    <pre><code>def build_vector_stores(embedded_docs: list, raw_docs: list): ...</code></pre>
  </ul>

  <h3>7. EnsembleRetriever Setup</h3>
  <ul>
    <li>Combine FAISS + Chroma with weight tuning</li>
    <li><a href="https://python.langchain.com/docs/how_to/ensemble_retriever/" target="_blank">EnsembleRetriever Docs</a></li>
    <pre><code>def get_ensemble_retriever() -> EnsembleRetriever: ...</code></pre>
  </ul>

  <h3>8. Query, Filter, and Score</h3>
  <ul>
    <li>Run queries + apply cosine similarity threshold</li>
    <li><a href="https://python.langchain.com/docs/modules/data_connection/retrievers/" target="_blank">Retriever Querying</a></li>
    <pre><code>def retrieve_relevant_chunks(query: str, retriever, threshold: float = 0.8) -> list: ...</code></pre>
  </ul>

  <h3>9. Reranking with Small Model</h3>
  <ul>
    <li>Use Ollama 2B to rerank top results</li>
    <li><a href="https://python.langchain.com/docs/guides/retrievers/re-ranking" target="_blank">Reranking Guide</a></li>
    <pre><code>def rerank_documents(query: str, candidates: list) -> dict: ...</code></pre>
  </ul>

  <h3>10. Final Answer Decision</h3>
  <ul>
    <li>Return <code>knowledge_base</code> or fall back to <code>ollama.chat()</code></li>
    <li><a href="https://python.langchain.com/docs/modules/model_io/output_parsers/" target="_blank">LangChain Output Parsers</a></li>
    <pre><code>def route_final_answer(query: str, candidates: list) -> dict: ...</code></pre>
  </ul>

  <h3>11. Save Metadata</h3>
  <ul>
    <li>Save extracted keywords, unanswered prompts</li>
    <pre><code>def save_keywords_and_logs(keywords: list, unanswered: list): ...</code></pre>
  </ul>


  <hr />

  <h2>ğŸ§± 1. Setup & Environment</h2>

<p>
  Before anything else, we need to prepare our environment to support the full RAG pipeline. This includes installing core dependencies, importing required libraries, and preparing the <code>Stores/</code> directory to hold our persistent vector databases (FAISS and Chroma).
</p>

<h3>ğŸ“¦ Required Dependencies</h3>

<ul>
  <li><code>langchain==0.1.13</code> â€“ Framework for chaining LLMs, vector search, and document management</li>
  <li><code>faiss-cpu==1.7.4</code> â€“ Vector similarity search engine used for dense semantic retrieval</li>
  <li><code>chromadb==0.4.22</code> â€“ Lightweight keyword/BM25-style vector database for sparse retrieval</li>
  <li><code>ollama==0.1.8</code> â€“ Interface for local LLaMA3-based models (inference and embedding)</li>
  <li><code>tiktoken==0.5.1</code> â€“ Tokenization utility for model-compatible chunking</li>
  <li><code>openai==1.12.0</code> â€“ Required internally by LangChain for embedding API compatibility</li>
  <li><code>python-dotenv==1.0.1</code> â€“ Load secrets and model names from environment variables</li>
  <li><code>nltk>=3.8.1</code> â€“ (Optional) for text preprocessing and keyword utility</li>
  <li><code>scikit-learn</code> â€“ Used internally for similarity math (e.g., cosine scores)</li>
  <li><code>transformers</code> â€“ (Optional) for reranking or small-model processing if needed</li>
</ul>

<h3>ğŸ”§ Installation</h3>

<pre><code>pip install langchain==0.1.13 faiss-cpu==1.7.4 chromadb==0.4.22 \ 
    tiktoken==0.5.1 openai==1.12.0 python-dotenv==1.0.1 \
    nltk scikit-learn transformers ollama==0.1.8
</code></pre>

<h3>ğŸ—‚ï¸ Directory Structure</h3>

<p>
  Ensure your project includes a <code>Stores/</code> folder where FAISS and Chroma index files will be persisted between runs.
</p>

<pre><code>llama_api_backend/
â”œâ”€â”€ Stores/
â”‚   â”œâ”€â”€ faiss_index/
â”‚   â””â”€â”€ chroma_index/
â”œâ”€â”€ semantic_search.ipynb
â”œâ”€â”€ app/
â”‚   â””â”€â”€ ...
</code></pre>

<h3>ğŸ” Function Template</h3>

<pre><code>def setup_environment():
    import os

    # Create necessary directories for vector stores
    os.makedirs("Stores/faiss_index", exist_ok=True)
    os.makedirs("Stores/chroma_index", exist_ok=True)

    print("âœ… Environment ready â€“ vector store folders created.")
</code></pre>

<h3>ğŸ”— Helpful Links</h3>
<ul>
  <li><a href="https://python.langchain.com/docs/get_started/installation" target="_blank">LangChain Installation Guide</a></li>
  <li><a href="https://python.langchain.com/docs/integrations/vectorstores/faiss" target="_blank">FAISS Vector Store Docs</a></li>
  <li><a href="https://python.langchain.com/docs/integrations/vectorstores/chroma" target="_blank">ChromaDB Integration Guide</a></li>
</ul>


<hr />

<h2>ğŸ” 2. Keyword Extraction</h2>

<p>
  In this step, we extract <strong>core keywords or named entities</strong> from the userâ€™s query using a lightweight Ollama model (e.g., 2B). This helps drive downstream processes like:
</p>

<ul>
  <li>Optional BM25-style keyword retrieval (via ChromaDB)</li>
  <li>UI suggestions for pre-populated follow-up questions in WordPress</li>
  <li>Future analytics or relevance scoring</li>
</ul>

<h3>ğŸ¤– Why Use a Model for Keywords?</h3>

<p>
  Instead of relying on brittle rule-based extractors, we use a **small LLaMA model via Ollama** to semantically extract high-value tokens, even across diverse domains. This gives us:
</p>

<ul>
  <li>More intelligent phrasing (e.g., detecting â€œauto insuranceâ€ vs â€œautoâ€ + â€œinsuranceâ€)</li>
  <li>Fewer false positives than pure TF-IDF or regex</li>
  <li>A consistent JSON structure for downstream tools</li>
</ul>

<h3>ğŸ§  System Prompt</h3>

<p>
  The following system prompt guides the model to extract 2â€“5 of the most important keywords or named entities:
</p>

<pre><code>{
  "role": "system",
  "content": "You are a helpful assistant that extracts 2 to 5 meaningful keywords or named entities from a user prompt. Return them as a Python list of lowercase strings. Avoid stopwords, articles, and general terms."
}
</code></pre>

<h3>ğŸ” Function Template</h3>

<pre><code>from ollama import chat

def extract_keywords_from_prompt(prompt: str) -> list[str]:
    system_prompt = {
        "role": "system",
        "content": (
            "You are a helpful assistant that extracts 2 to 5 meaningful keywords or named entities "
            "from a user prompt. Return them as a Python list of lowercase strings. "
            "Avoid stopwords, articles, and general terms."
        )
    }

    user_prompt = {"role": "user", "content": prompt}

    try:
        response = chat(
            model="llama2:2b",  # or any small/fast Ollama model
            messages=[system_prompt, user_prompt]
        )

        raw_output = response.get("message", {}).get("content", "[]")
        keywords = eval(raw_output.strip()) if isinstance(raw_output, str) else []

        # Optional: basic sanitization
        return [kw.lower().strip() for kw in keywords if isinstance(kw, str)]

    except Exception as e:
        print(f"âŒ Keyword extraction failed: {e}")
        return []
</code></pre>

<h3>ğŸ’¾ Optional Logging</h3>

<p>
  Store the extracted keywords in a local file (e.g., <code>extracted_keywords.json</code>) for:
</p>

<ul>
  <li>Debugging</li>
  <li>Training future ranking models</li>
  <li>WordPress frontend suggestions</li>
</ul>

<pre><code>import json

def save_keywords_to_json(prompt: str, keywords: list[str]):
    log_path = "Stores/extracted_keywords.json"

    try:
        existing = []
        if os.path.exists(log_path):
            with open(log_path, "r") as f:
                existing = json.load(f)

        existing.append({"prompt": prompt, "keywords": keywords})

        with open(log_path, "w") as f:
            json.dump(existing, f, indent=2)

    except Exception as e:
        print(f"âš ï¸ Could not save keywords: {e}")
</code></pre>

<h3>ğŸ”— References</h3>
<ul>
  <li><a href="https://ollama.com/library" target="_blank">Ollama Model Library</a></li>
  <li><a href="https://python.langchain.com/docs/modules/model_io/llms/" target="_blank">LangChain LLMs (Optional)</a></li>
</ul>

<hr />

<h2>ğŸ“¥ 3. Load & Index Q&A Data (FAISS + Chroma)</h2>

<p>
  In this step, weâ€™ll use a custom class to handle loading, chunking, embedding, and indexing our Q&A JSON data into two vector databases:
</p>

<ul>
  <li><strong>FAISS</strong> â€“ for dense semantic search using vector embeddings</li>
  <li><strong>ChromaDB</strong> â€“ for keyword/BM25-based sparse retrieval</li>
</ul>

<h3>ğŸ§  Class-Based Design</h3>

<p>
  Weâ€™ll use a Python class called <code>QADocumentIndexer</code> to encapsulate all steps:
</p>

<ul>
  <li>Load and parse Q&A JSON</li>
  <li>Optionally chunk text (for FAISS)</li>
  <li>Embed and store in FAISS</li>
  <li>Store raw documents in Chroma for BM25</li>
</ul>

<hr />

<h3>ğŸ“¦ QADocumentIndexer (Full Code)</h3>

<pre><code>import os
import json
from typing import List
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS, Chroma
from langchain.embeddings import OllamaEmbeddings

class QADocumentIndexer:
    def __init__(self, json_path: str,
                 faiss_dir: str = "Stores/faiss_index",
                 chroma_dir: str = "Stores/chroma_index",
                 embedding_model: str = "nomic-embed-text"):
        self.json_path = json_path
        self.faiss_dir = faiss_dir
        self.chroma_dir = chroma_dir
        self.embedding = OllamaEmbeddings(model=embedding_model)
        self.raw_docs: List[Document] = []
        self.chunked_docs: List[Document] = []
        os.makedirs(faiss_dir, exist_ok=True)
        os.makedirs(chroma_dir, exist_ok=True)

    def load_qa_json(self):
        with open(self.json_path, "r", encoding="utf-8") as f:
            qa_data = json.load(f)
        self.raw_docs = [
            Document(
                page_content=entry["answer"],
                metadata={
                    "questions": entry.get("questions", []),
                    "tags": entry.get("tags", []),
                    "confidence": entry.get("confidence", 0.0),
                    "source": entry.get("source", "unknown"),
                    "approved": entry.get("approved", False)
                }
            ) for entry in qa_data
        ]
        print(f"âœ… Loaded {len(self.raw_docs)} raw documents.")

    def chunk_documents(self, chunk_size=512, chunk_overlap=50):
        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        self.chunked_docs = splitter.split_documents(self.raw_docs)
        print(f"âœ‚ï¸ Chunked into {len(self.chunked_docs)} documents for FAISS.")

    def index_to_faiss(self):
        if not self.chunked_docs:
            raise ValueError("No chunks. Run chunk_documents() first.")
        faiss_store = FAISS.from_documents(self.chunked_docs, self.embedding)
        faiss_store.save_local(self.faiss_dir)
        print(f"âœ… FAISS index saved to: {self.faiss_dir}")

    def index_to_chroma(self):
        if not self.raw_docs:
            raise ValueError("No raw docs. Run load_qa_json() first.")
        chroma_store = Chroma.from_documents(
            documents=self.raw_docs,
            embedding=None,  # No need for embeddings in BM25
            persist_directory=self.chroma_dir,
            collection_name="qa_bm25"
        )
        chroma_store.persist()
        print(f"âœ… ChromaBM25 index saved to: {self.chroma_dir}")

    def run_all(self):
        self.load_qa_json()
        self.chunk_documents()
        self.index_to_faiss()
        self.index_to_chroma()
        print("ğŸš€ Full Q&A indexing completed.")
</code></pre>

<h3>ğŸ§ª Usage Example</h3>

<pre><code># One-liner
indexer = QADocumentIndexer("data/sample_qa_pairs.json")
indexer.run_all()

# Or step-by-step
indexer = QADocumentIndexer("data/sample_qa_pairs.json")
indexer.load_qa_json()
indexer.chunk_documents()
indexer.index_to_faiss()
indexer.index_to_chroma()
</code></pre>

<h3>ğŸ“ Result</h3>
<p>This class will create the following persistent folders:</p>

<ul>
  <li><code>Stores/faiss_index/</code> â†’ used by FAISS retriever</li>
  <li><code>Stores/chroma_index/</code> â†’ used by Chroma (BM25 keyword search)</li>
</ul>

<h3>ğŸ”— Next Step</h3>
<p>Move on to <strong>Chapter 4</strong> to set up <code>EnsembleRetriever</code> and perform real hybrid searches.</p>


</body>
</html>